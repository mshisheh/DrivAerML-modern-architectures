# Download DrivAerNet++ Surface Data Only

For surface pressure prediction, you only need the **surface mesh** and **pressure field** files, not the 406 GB volume data.

## What You Need (Much Smaller!)

| Data Type | Size | Priority | Files |
|-----------|------|----------|-------|
| **Surface Pressure** | ~20-30 GB | âœ… **REQUIRED** | `.vtp` or `.vtk` files |
| **Surface Meshes (STL)** | ~5-10 GB | âœ… **REQUIRED** | `.stl` files |
| **Coefficients (CSV)** | <1 MB | âœ… **REQUIRED** | `.csv` files |
| Volume Data | 406 GB | âŒ **NOT NEEDED** | `.vtu` files |

**Total needed: ~30-40 GB** instead of 406 GB!

## Download Instructions

### Step 1: Go to Harvard Dataverse

**URL**: https://dataverse.harvard.edu/dataverse/DrivAerNet

### Step 2: Look for These Specific Files

On the dataverse page, look for archives named something like:

- `Surface_Pressure.zip` or `PressureVTK.zip` (~20-30 GB)
- `Surface_Meshes.zip` or `STL_Combined.zip` (~5-10 GB)
- `Coefficients.csv` (<1 MB)

**Note**: The exact filenames may vary. Look for "surface" or "boundary" in the name.

### Step 3: Download via Browser or wget

#### Option A: Browser Download (Simple)
1. Click on each file
2. Click "Download"
3. Wait for download to complete

#### Option B: wget/curl (Faster, resumable)

Once you have the direct URLs from Harvard Dataverse, use:

```powershell
# Install wget for Windows if needed
# winget install GnuWin32.Wget

# Download surface pressure data (replace URL with actual link from dataverse)
wget -c "https://dataverse.harvard.edu/api/access/datafile/XXXXX" -O Surface_Pressure.zip

# Download surface meshes
wget -c "https://dataverse.harvard.edu/api/access/datafile/YYYYY" -O Surface_Meshes.zip
```

The `-c` flag allows resume if download is interrupted.

### Step 4: Extract to Your Data Directory

```powershell
# Create data directory
cd C:\Learning\Scientific\CARBENCH\DrivAerNet
New-Item -ItemType Directory -Force -Path "data\PressureVTK"
New-Item -ItemType Directory -Force -Path "data\STL"

# Extract (assuming you have 7zip or similar)
# Adjust paths based on where you downloaded
7z x Surface_Pressure.zip -o"data\PressureVTK"
7z x Surface_Meshes.zip -o"data\STL"
```

### Step 5: Verify Data Structure

After extraction, you should have:

```
C:\Learning\Scientific\CARBENCH\DrivAerNet\
â”œâ”€â”€ data\
â”‚   â”œâ”€â”€ PressureVTK\
â”‚   â”‚   â”œâ”€â”€ DrivAer_F_D_WM_WW_0001.vtp
â”‚   â”‚   â”œâ”€â”€ DrivAer_F_D_WM_WW_0002.vtp
â”‚   â”‚   â””â”€â”€ ... (~8,150 files)
â”‚   â”œâ”€â”€ STL\
â”‚   â”‚   â”œâ”€â”€ DrivAer_F_D_WM_WW_0001.stl
â”‚   â”‚   â”œâ”€â”€ DrivAer_F_D_WM_WW_0002.stl
â”‚   â”‚   â””â”€â”€ ... (~8,150 files)
â”‚   â”œâ”€â”€ DrivAerNetPlusPlus_Cd_8k.csv
â”‚   â””â”€â”€ DrivAerNetPlusPlus_Areas.csv
â””â”€â”€ train_val_test_splits\
    â”œâ”€â”€ train_designs.txt
    â”œâ”€â”€ val_designs.txt
    â””â”€â”€ test_designs.txt
```

## Alternative: Start with a Subset

If even 30-40 GB is too much, you can:

### Option 1: Download Only Validation Set (~800 designs â‰ˆ 3-4 GB)

1. Download full surface data
2. Extract only files matching validation set:

```powershell
# Read validation design IDs
$val_ids = Get-Content "train_val_test_splits\val_designs.txt"

# Copy only validation files to a subset directory
foreach ($id in $val_ids) {
    Copy-Item "data\PressureVTK\$id.vtp" -Destination "data_subset\PressureVTK\" -ErrorAction SilentlyContinue
    Copy-Item "data\STL\$id.stl" -Destination "data_subset\STL\" -ErrorAction SilentlyContinue
}
```

### Option 2: Contact Dataset Authors

If Harvard Dataverse doesn't have surface-only downloads separated, you can:

ðŸ“§ Email: **mohamed.elrefaie@mit.edu**
Subject: "DrivAerNet++ Surface-Only Data Request"

Ask if they have a surface-only subset available for download.

## What You DON'T Need

âŒ **Skip these to save space:**
- Volume files (`.vtu`) - 406 GB
- Any file with "volume" in the name
- Folders named "volume" or "volumetric"
- Any 3D grid/voxel data

## Next Steps After Download

Once you have the surface data, you'll need to:

1. **Convert VTP â†’ NPY** format (for your models)
2. **Preprocess**: Normalize coordinates, compute areas
3. **Verify**: Check against train/val/test splits

Would you like me to create a preprocessing script to convert the VTP files to the `.npy` format your models expect?
